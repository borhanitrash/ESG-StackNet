{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":14167867,"datasetId":9030913,"databundleVersionId":14958755}],"dockerImageVersionId":31286,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==============================================================================\n# Cell: Simple Baseline Model Comparison (Overall Sector - Subsets)\n# ==============================================================================\nprint(\"--- Starting Baseline Model Comparison for Reviewer Response ---\")\n\nimport pandas as pd\nimport numpy as np\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# --- Configuration ---\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 1000)\n\n# --- 1. Load Data ---\nprint(\"1. Loading Data...\")\ntry:\n    df = pd.read_csv('/kaggle/input/esg-dataset/Environmental.csv', low_memory=False)\n    print(f\"   Dataset loaded successfully. Shape: {df.shape}\")\nexcept FileNotFoundError:\n    print(\"Error: 'Environmental.csv' not found. Please check the file path.\")\n    exit()\n\n# --- 2. Feature Engineering (Identical to Proposed Method) ---\nprint(\"2. Replicating Feature Engineering...\")\ndf['corpgov_cganalyticboardfemale'] = pd.to_numeric(df['corpgov_cganalyticboardfemale'], errors='coerce')\ndf['corpgov_boardsize'] = pd.to_numeric(df['corpgov_boardsize'], errors='coerce')\n\n# Calculate Blau Index\nfemale_prop = df['corpgov_cganalyticboardfemale'] / 100\nmale_prop = 1 - female_prop\ndf['Blau_Index'] = 1 - (female_prop**2 + male_prop**2)\ndf['Blau_Index'].fillna(0, inplace=True)\n\n# Calculate Critical Mass Dummies\nnum_female_directors = (female_prop * df['corpgov_boardsize']).round()\ndf['has_1_woman'] = (num_female_directors == 1).astype(int)\ndf['has_2_women'] = (num_female_directors == 2).astype(int)\ndf['has_3plus_women'] = (num_female_directors >= 3).astype(int)\n\n# --- 3. Define Constants & Preprocessing Functions ---\nTARGET_COL_NAME = 'corpgov_tresgscore'\nGENDER_COLS = ['corpgov_cganalyticboardfemale', 'Blau_Index', 'has_1_woman', 'has_2_women', 'has_3plus_women']\nALL_POSSIBLE_TARGETS = ['corpgov_tresgscore', 'corpgov_environmentpillarscore', 'corpgov_socialpillarscore', 'corpgov_governancepillarscore']\n\ndef preprocess_data(X_train, X_test, y_train, y_test):\n    X_train_proc, X_test_proc = X_train.copy(), X_test.copy()\n    y_train_proc, y_test_proc = y_train.copy(), y_test.copy()\n    \n    # Target Imputation\n    median_val = y_train_proc[TARGET_COL_NAME].median()\n    if pd.isna(median_val): median_val = df[TARGET_COL_NAME].median()\n    y_train_proc[TARGET_COL_NAME] = y_train_proc[TARGET_COL_NAME].fillna(median_val)\n    y_test_proc[TARGET_COL_NAME] = y_test_proc[TARGET_COL_NAME].fillna(median_val)\n        \n    # Drop columns logic\n    missing_percent = X_train_proc.isnull().sum() / len(X_train_proc)\n    cols_to_drop_missing = missing_percent[missing_percent > 0.4].index\n    cols_to_drop_other = [col for col in X_train_proc.columns if ('iden' in str(col)) or ('year' in str(col) and X_train_proc[col].dtype == 'object') or X_train_proc[col].nunique() <= 1]\n    cols_to_drop = list(set(cols_to_drop_missing) | set(cols_to_drop_other))\n    cols_to_drop = [col for col in cols_to_drop if col not in GENDER_COLS]\n    X_train_proc = X_train_proc.drop(columns=cols_to_drop)\n    X_test_proc = X_test_proc.drop(columns=cols_to_drop, errors='ignore')\n    \n    # Numerical Imputation\n    numerical_cols = X_train_proc.select_dtypes(include=np.number).columns.tolist()\n    categorical_cols = X_train_proc.select_dtypes(exclude=np.number).columns.tolist()\n    \n    num_imputer = SimpleImputer(strategy='median')\n    X_train_proc[numerical_cols] = num_imputer.fit_transform(X_train_proc[numerical_cols])\n    X_test_proc[numerical_cols] = num_imputer.transform(X_test_proc[numerical_cols])\n    \n    # Categorical Encoding\n    for col in categorical_cols:\n        mode_val = X_train_proc[col].mode()[0]\n        X_train_proc[col] = X_train_proc[col].fillna(mode_val)\n        X_test_proc[col] = X_test_proc[col].fillna(mode_val)\n        le = LabelEncoder().fit(pd.concat([X_train_proc[col].astype(str), X_test_proc[col].astype(str)]).unique())\n        X_train_proc[col] = le.transform(X_train_proc[col].astype(str))\n        X_test_proc[col] = le.transform(X_test_proc[col].astype(str))\n        \n    train_cols, test_cols = X_train_proc.columns, X_test_proc.columns\n    shared_cols = list(set(train_cols) & set(test_cols))\n    return X_train_proc[shared_cols], X_test_proc[shared_cols], y_train_proc, y_test_proc\n\ndef get_top_features_combined(X_train, y_train_target, n_features=25):\n    # This ensures Baseline models use the exact same input features logic as the Complex model\n    y = y_train_target.values.ravel()\n    rf = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1, max_depth=10).fit(X_train, y)\n    importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n    \n    scaler_fs = StandardScaler()\n    X_train_scaled_fs = scaler_fs.fit_transform(X_train)\n    \n    # Handle KBest\n    selector_kbest = SelectKBest(f_regression, k='all').fit(X_train, y)\n    \n    # Handle Lasso\n    lasso = Lasso(alpha=0.1, max_iter=1000, random_state=42).fit(X_train_scaled_fs, y)\n    \n    combined = (pd.Series(selector_kbest.scores_, index=X_train.columns).fillna(0).rank(pct=True) +\n                pd.Series(np.abs(lasso.coef_), index=X_train.columns).fillna(0).rank(pct=True) +\n                importances.rank(pct=True))\n    return combined.nlargest(n_features).index.tolist()\n\n# --- 4. Global Feature Selection (Done once for consistency) ---\nprint(\"3. Performing global feature selection (same as proposed method)...\")\nX_full = df.drop(columns=ALL_POSSIBLE_TARGETS, errors='ignore')\ny_full = df[[TARGET_COL_NAME]]\nX_train_full, _, y_train_full, _ = train_test_split(X_full, y_full, test_size=0.2, random_state=42)\n\n# Preprocess Global\nX_train_proc_full, _, y_train_proc_full, _ = preprocess_data(X_train_full, X_train_full.copy(), y_train_full, y_train_full.copy())\ntop_25_features = get_top_features_combined(X_train_proc_full, y_train_proc_full, n_features=25)\nfinal_feature_set = list(dict.fromkeys(GENDER_COLS + [f for f in top_25_features if f not in GENDER_COLS]))\nprint(f\"   Identified {len(final_feature_set)} features for modeling.\")\n\n# --- 5. Main Analysis Loop ---\nprint(\"\\n4. Running Baseline Models for Overall Sector Subsets...\")\n\n# Define Simple Models\nbaseline_models = {\n    \"Linear Regression\": LinearRegression(),\n    \"Decision Tree\": DecisionTreeRegressor(max_depth=5, random_state=42),\n    \"KNN\": KNeighborsRegressor(n_neighbors=5)\n}\n\n# Define Scenarios\nscenarios = {\"Overall\": df}\n\nfor scenario_name, scenario_df in scenarios.items():\n    \n    # Create the 3 specific subsets\n    subsets = {\n        \"All Board Types\": scenario_df.copy(),\n        \"All-Men Board\": scenario_df[scenario_df['corpgov_cganalyticboardfemale'] == 0].copy(),\n        \"Diverse Board\": scenario_df[scenario_df['corpgov_cganalyticboardfemale'] > 0].copy()\n    }\n    \n    for subset_name, subset_df in subsets.items():\n        print(f\"\\nProcessing: {scenario_name} - {subset_name}\")\n        print(f\"   Data Shape: {subset_df.shape}\")\n        \n        if subset_df.shape[0] < 50:\n            print(\"   Not enough data to model. Skipping.\")\n            continue\n            \n        # 1. Split\n        X = subset_df.drop(columns=ALL_POSSIBLE_TARGETS, errors='ignore')\n        y = subset_df[[TARGET_COL_NAME]]\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # 2. Preprocess\n        X_train_proc, X_test_proc, y_train_proc, y_test_proc = preprocess_data(X_train, X_test, y_train, y_test)\n        \n        # 3. Select Features\n        available_features = [f for f in final_feature_set if f in X_train_proc.columns and f in X_test_proc.columns]\n        X_train_selected = X_train_proc[available_features]\n        X_test_selected = X_test_proc[available_features]\n        \n        # 4. Scale (Required for Linear Regression & KNN)\n        scaler = StandardScaler()\n        X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_selected), columns=available_features)\n        X_test_scaled = pd.DataFrame(scaler.transform(X_test_selected), columns=available_features)\n        \n        # 5. Train & Evaluate\n        subset_results = []\n        for model_name, model in baseline_models.items():\n            # Fit\n            model.fit(X_train_scaled, y_train_proc.values.ravel())\n            \n            # Predict\n            y_pred = model.predict(X_test_scaled)\n            \n            # Metrics\n            rmse = np.sqrt(mean_squared_error(y_test_proc, y_pred))\n            mae = mean_absolute_error(y_test_proc, y_pred)\n            r2 = r2_score(y_test_proc, y_pred)\n            \n            subset_results.append({\n                'Model': model_name,\n                'RMSE': rmse,\n                'MAE': mae,\n                'R2 Score': r2\n            })\n            \n        # 6. Show Results\n        results_df = pd.DataFrame(subset_results).set_index('Model').sort_values('RMSE')\n        print(f\"--- Results for {scenario_name} - {subset_name} ---\")\n        print(results_df.round(4))\n        print(\"-\" * 60)\n\nprint(\"\\n--- Baseline Model Comparison Complete ---\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-22T10:56:10.056126Z","iopub.execute_input":"2026-02-22T10:56:10.056423Z","iopub.status.idle":"2026-02-22T10:57:19.267739Z","shell.execute_reply.started":"2026-02-22T10:56:10.056404Z","shell.execute_reply":"2026-02-22T10:57:19.266664Z"}},"outputs":[{"name":"stdout","text":"--- Starting Baseline Model Comparison for Reviewer Response ---\n1. Loading Data...\n   Dataset loaded successfully. Shape: (21496, 509)\n2. Replicating Feature Engineering...\n3. Performing global feature selection (same as proposed method)...\n   Identified 30 features for modeling.\n\n4. Running Baseline Models for Overall Sector Subsets...\n\nProcessing: Overall - All Board Types\n   Data Shape: (21496, 513)\n--- Results for Overall - All Board Types ---\n                     RMSE     MAE  R2 Score\nModel                                      \nLinear Regression  1.5041  1.0392    0.9899\nDecision Tree      1.7706  1.2520    0.9860\nKNN                2.7960  1.9564    0.9652\n------------------------------------------------------------\n\nProcessing: Overall - All-Men Board\n   Data Shape: (3181, 513)\n--- Results for Overall - All-Men Board ---\n                     RMSE     MAE  R2 Score\nModel                                      \nLinear Regression  1.3432  0.9339    0.9939\nDecision Tree      1.7051  1.2375    0.9902\nKNN                3.0426  2.3364    0.9689\n------------------------------------------------------------\n\nProcessing: Overall - Diverse Board\n   Data Shape: (14670, 513)\n--- Results for Overall - Diverse Board ---\n                     RMSE     MAE  R2 Score\nModel                                      \nLinear Regression  1.7239  1.2690    0.9879\nDecision Tree      1.9269  1.3941    0.9849\nKNN                3.1391  2.3947    0.9599\n------------------------------------------------------------\n\n--- Baseline Model Comparison Complete ---\n","output_type":"stream"}],"execution_count":7}]}